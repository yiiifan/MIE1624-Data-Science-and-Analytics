{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decisions Trees are mainly used to solve classification problems. This notebook will cover how a decision tree is created, and will show how to plot the results of a decision tree.\n",
    "\n",
    "This is based on sample code from *Data Science from Scratch* by Joel Grus, O'Reilly Media, 2015.\n",
    "\n",
    "<u>**Problem**</u>\n",
    "\n",
    "The VP provides you with the interviewee data, consisting of (per your specification) pairs $(input, label)$, where each input is a dict of candidate attributes, and each label is either $True$ (the candidate interviewed well) or $False$ (the candidate interviewed poorly). In particular, you are provided with each candidate’s level, their preferred language, whether they are active on Twitter, and whether they have a PhD.\n",
    "\n",
    "<u>**Solution**</u>\n",
    "\n",
    "Our tree will consist of decision nodes (which ask a question and direct us differently depending on the answer) and leaf nodes (which give us a prediction). We will build it using the relatively simple ID3 algorithm, which operates in the following manner. Let’s say we’re given some labeled data, and a list of attributes to consider branching on.\n",
    "- If the data all have the same label, then create a leaf node that predicts that label and then stop. \n",
    "- If the list of attributes is empty (i.e., there are no more possible questions to ask), then create a leaf node that predicts the most common label and then stop. \n",
    "- Otherwise, try partitioning the data by each of the attributes.\n",
    "- Choose the partition with the lowest partition entropy.\n",
    "- Recur on each partitioned subset using the remaining attributes.\n",
    "\n",
    "This is what’s known as a “greedy” algorithm because, at each step, it chooses the most immediately best option. Given a data set, there may be a better tree with a worse-looking first move. If so, this algorithm won’t find it. Nonetheless, it is relatively easy to understand and implement, which makes it a good place to begin exploring decision trees.\n",
    "\n",
    "<u>**Table of Contents**</u>\n",
    " - <a href=\"#NF\">Necessary Functions</a>\n",
    " - <a href=\"#P\">Find Partition with Least Entropy</a>\n",
    " - <a href=\"#BT\">Building a Tree</a>\n",
    " - <a href=\"#MP\">Making Predictions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"NF\">Necessary Functions</a>\n",
    "Here we will define necessary functions that we will use below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `entropy`\n",
    "This function will compute and return the entropy, given a list of class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `class_probabilities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_entropy(labeled_data):        \n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `partition_entropy`\n",
    "Partitions passed data into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    \n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `group_by`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item \n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `partition_by`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `partition_entropy_by`\n",
    "Uses `partition_entropy` to partition dataset, and then calculates entropy for each partition. Return both the partitions and the entropy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs,attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"        \n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `classify`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "    \n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "   \n",
    "    # otherwise find the correct subtree\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None              # we'll use the None subtree\n",
    "    \n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input)     # and use it to classify the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `build_tree_id3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass, \n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "        \n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "                            \n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "        key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates \n",
    "                      if a != best_attribute]\n",
    "    \n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.items() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `forest_classify`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'lang': 'Java', 'level': 'Senior', 'phd': 'no', 'tweets': 'no'}, False),\n",
       " ({'lang': 'Java', 'level': 'Senior', 'phd': 'yes', 'tweets': 'no'}, False),\n",
       " ({'lang': 'Python', 'level': 'Mid', 'phd': 'no', 'tweets': 'no'}, True),\n",
       " ({'lang': 'Python', 'level': 'Junior', 'phd': 'no', 'tweets': 'no'}, True),\n",
       " ({'lang': 'R', 'level': 'Junior', 'phd': 'no', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'R', 'level': 'Junior', 'phd': 'yes', 'tweets': 'yes'}, False),\n",
       " ({'lang': 'R', 'level': 'Mid', 'phd': 'yes', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'Python', 'level': 'Senior', 'phd': 'no', 'tweets': 'no'}, False),\n",
       " ({'lang': 'R', 'level': 'Senior', 'phd': 'no', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'Python', 'level': 'Junior', 'phd': 'no', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'Python', 'level': 'Senior', 'phd': 'yes', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'Python', 'level': 'Mid', 'phd': 'yes', 'tweets': 'no'}, True),\n",
       " ({'lang': 'Java', 'level': 'Mid', 'phd': 'no', 'tweets': 'yes'}, True),\n",
       " ({'lang': 'Python', 'level': 'Junior', 'phd': 'yes', 'tweets': 'no'}, False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
    "    ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
    "    ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
    "    ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
    "    ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
    "    ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
    "    ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
    "    ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
    "    ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
    "    ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
    "    ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s manually go through these steps on the interviewee data set. The data set has both True and False labels, and we have four attributes we can split on: `lang`, `level`, `phd`, and `tweets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['level', 'lang', 'tweets', 'phd']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_attrs = [key for key in data[0][0].keys()]\n",
    "remaining_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also generate a dictionary containing all the unique elements for each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level ['Mid', 'Senior', 'Junior']\n",
      "lang ['Python', 'R', 'Java']\n",
      "tweets ['no', 'yes']\n",
      "phd ['no', 'yes']\n"
     ]
    }
   ],
   "source": [
    "valsDict = {}\n",
    "for key in data[0][0].keys():\n",
    "    vals = []\n",
    "    for d in data:\n",
    "        vals.append(d[0][key])\n",
    "    vals = list(set(vals))\n",
    "    \n",
    "    valsDict[key] = vals\n",
    "\n",
    "for k,v in valsDict.items():\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"P\">Find Partition with Least Entropy</a>\n",
    "So our first step will be to find the partition with the least entropy. Function `partition_by()` does the partitioning and function `partition_entropy_by()` uses the partitioning to compute entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n",
      "\n",
      "Attribute to split on: level\n"
     ]
    }
   ],
   "source": [
    "min_entropy = 1\n",
    "split_attr = ''\n",
    "for attribute in remaining_attrs:\n",
    "    attr_entropy = partition_entropy_by(data, attribute)\n",
    "    print (attribute, attr_entropy)\n",
    "    if attr_entropy < min_entropy:\n",
    "        split_attr = attribute\n",
    "        min_entropy = attr_entropy\n",
    "        \n",
    "print('\\nAttribute to split on:',split_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split on the attribute that minimizes entropy, in this case `level`. We now need to make a subtree for each possible level value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will drop `level` as a potential attribute to split on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Attributes: ['lang', 'tweets', 'phd']\n"
     ]
    }
   ],
   "source": [
    "remaining_attrs.remove(split_attr)\n",
    "print(\"Remaining Attributes:\", remaining_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will compute the score distribution for each level type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of level attribute:\n",
      "\n",
      "Mid (T:F): 4:0\n",
      "Senior (T:F): 2:3\n",
      "Junior (T:F): 3:2\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of {} attribute:\\n\".format(split_attr))\n",
    "for val in valsDict[split_attr]:\n",
    "    T_ctr = 0\n",
    "    F_ctr = 0\n",
    "    for d in data:\n",
    "        if d[0][split_attr] == val:\n",
    "            if d[1]:\n",
    "                T_ctr+=1\n",
    "            else:\n",
    "                F_ctr+=1\n",
    "    print(\"{} (T:F): {}:{}\".format(val, T_ctr, F_ctr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ever $mid$ `label` candidate is labelled $True$, which means that the $mid$ subtree is simply a leaf node predicting $True$. For $senior$ and $junior$ candidates, we have a mix of $True$ and $False$, so we need another attribute to split on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level == Senior\n",
      "\n",
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n",
      "\n",
      "Attribute to split on: tweets\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [(d, label) for d, label in data if d[\"level\"] == \"Senior\"]\n",
    "min_entropy = 1\n",
    "lvl_senior_split_attr = ''\n",
    "print ('level == Senior\\n')\n",
    "for attribute in remaining_attrs:\n",
    "    attr_entropy =  partition_entropy_by(senior_inputs, attribute)\n",
    "    print (attribute, attr_entropy)\n",
    "    if attr_entropy < min_entropy:\n",
    "        lvl_senior_split_attr = attribute\n",
    "        min_entropy = attr_entropy\n",
    "        \n",
    "print('\\nAttribute to split on:',lvl_senior_split_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that our next split should be on `tweets` for the $senior$ `level`, which results in a zero-entropy partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of tweets attribute with level = Senior:\n",
      "\n",
      "no (T:F): 0:3\n",
      "yes (T:F): 2:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of {} attribute with level = {}:\\n\".format(lvl_senior_split_attr, 'Senior'))\n",
    "for val in valsDict[lvl_senior_split_attr]:\n",
    "    T_ctr = 0\n",
    "    F_ctr = 0\n",
    "    for d in data:\n",
    "        if d[0]['level'] == 'Senior':\n",
    "            if d[0][lvl_senior_split_attr] == val:\n",
    "                if d[1]:\n",
    "                    T_ctr+=1\n",
    "                else:\n",
    "                    F_ctr+=1\n",
    "    print(\"{} (T:F): {}:{}\".format(val, T_ctr, F_ctr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that for $Senior$ `level` candidates, $yes$ `tweets` always result in $True$ while $no$ `tweets` always result in $False$.\n",
    "\n",
    "Now we do similarly with the $Junior$ `level`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level == Junior\n",
      "\n",
      "lang 0.9509775004326938\n",
      "tweets 0.9509775004326938\n",
      "phd 0.0\n",
      "\n",
      "Attribute to split on: phd\n"
     ]
    }
   ],
   "source": [
    "junior_inputs = [(d, label) for d, label in data if d[\"level\"] == \"Junior\"]\n",
    "min_entropy = 1\n",
    "lvl_junior_split_attr = ''\n",
    "print ('level == Junior\\n')\n",
    "for attribute in remaining_attrs:\n",
    "    attr_entropy =  partition_entropy_by(junior_inputs, attribute)\n",
    "    print (attribute, attr_entropy)\n",
    "    if attr_entropy < min_entropy:\n",
    "        lvl_junior_split_attr = attribute\n",
    "        min_entropy = attr_entropy\n",
    "        \n",
    "print('\\nAttribute to split on:',lvl_junior_split_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This shows us that our next split should be on `tweets` for the $junior$ `level`, which results in a zero-entropy partition again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of phd attribute with level = Junior:\n",
      "\n",
      "no (T:F): 3:0\n",
      "yes (T:F): 0:2\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of {} attribute with level = {}:\\n\".format(lvl_junior_split_attr, 'Junior'))\n",
    "for val in valsDict[lvl_junior_split_attr]:\n",
    "    T_ctr = 0\n",
    "    F_ctr = 0\n",
    "    for d in data:\n",
    "        if d[0]['level'] == 'Junior':\n",
    "            if d[0][lvl_junior_split_attr] == val:\n",
    "                if d[1]:\n",
    "                    T_ctr+=1\n",
    "                else:\n",
    "                    F_ctr+=1\n",
    "    print(\"{} (T:F): {}:{}\".format(val, T_ctr, F_ctr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that for $Junior$ `level` candidates, $yes$ `phd` always result in $False$ while $no$ `phd` always result in $True$.\n",
    "\n",
    "This routine is continued until all possible values are classified in a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"BT\">Building a Tree</a>\n",
    "Now that we’ve seen how the algorithm works, we would like to implement it more generally. This means we need to decide how we want to represent trees. We’ll use pretty much the most lightweight representation possible. We define a tree to be one of the following:\n",
    "- True\n",
    "- False\n",
    "- a tuple (attribute, subtree_dict)\n",
    "\n",
    "Here $True$ represents a leaf node that returns True for any input, $False$ represents a leaf node that returns False for any input, and a tuple represents a decision node that, for any input, finds its attribute value, and classifies the input using the corresponding subtree.\n",
    "\n",
    "There’s still the question of what to do if we encounter an unexpected (or missing) attribute value. What should our hiring tree do if it encounters a candidate whose level is “Intern”? We’ll handle this case by adding a None key that just predicts the most common label.\n",
    "\n",
    "All that’s left is to build the tree representation from our training data. Our hiring tree would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Building a Tree---\n",
      "\n",
      "level :\n",
      "  Senior\n",
      "    tweets :\n",
      "      no\n",
      "        False\n",
      "      yes\n",
      "        True\n",
      "      None\n",
      "        False\n",
      "  Mid\n",
      "    True\n",
      "  Junior\n",
      "    phd :\n",
      "      no\n",
      "        True\n",
      "      yes\n",
      "        False\n",
      "      None\n",
      "        True\n",
      "  None\n",
      "    True\n"
     ]
    }
   ],
   "source": [
    "print (\"---Building a Tree---\\n\")\n",
    "\n",
    "tree = build_tree_id3(data)\n",
    "\n",
    "lvl1, subtree1 = tree\n",
    "print (lvl1,':')\n",
    "\n",
    "for k,v in subtree1.items():\n",
    "    print('  {}'.format(k))\n",
    "    if type(v) != tuple:\n",
    "        print ('    {}'.format(v))\n",
    "    else:\n",
    "        lvl2, subtree2 = v\n",
    "        print('    {} :'.format(lvl2))\n",
    "        for k,v in subtree2.items():\n",
    "            print('      {}'.format(k))\n",
    "            if type(v) != tuple:\n",
    "                print ('        {}'.format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree we built, every leaf consisted entirely of $True$ inputs or entirely of $False$ inputs. This means that the tree predicts perfectly on the training data set. But we can also apply it to new data that wasn’t in the training set to classify a new input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"MP\">Making Predictions</a>\n",
    "We will now use our built tree to make predictions on the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: \n",
    "- level: Junior\n",
    "- lang: Java\n",
    "- tweets: Yes\n",
    "- phd: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'Junior', 'lang': 'Java', 'tweets': 'yes', 'phd': 'no'}\n",
      "Label: True\n"
     ]
    }
   ],
   "source": [
    "ex1 = {\n",
    "    'level':'Junior',\n",
    "    'lang':'Java',\n",
    "    'tweets':'yes',\n",
    "    'phd':'no'\n",
    "}\n",
    "\n",
    "print(ex1)\n",
    "\n",
    "label = classify(tree,ex1)\n",
    "\n",
    "print ('Label:',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: \n",
    "- level: Junior\n",
    "- lang: Java\n",
    "- tweets: Yes\n",
    "- phd: Yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'Junior', 'lang': 'Java', 'tweets': 'yes', 'phd': 'yes'}\n",
      "Label: False\n"
     ]
    }
   ],
   "source": [
    "ex2 = {\n",
    "    'level':'Junior',\n",
    "    'lang':'Java',\n",
    "    'tweets':'yes',\n",
    "    'phd':'yes'\n",
    "}\n",
    "\n",
    "print(ex2)\n",
    "\n",
    "label = classify(tree,ex2)\n",
    "\n",
    "print ('Label:',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Unexpected/missing data\n",
    "- level: Intern\n",
    "- tweets: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'Intern', 'tweets': 'no'}\n",
      "Label: True\n"
     ]
    }
   ],
   "source": [
    "ex3 = {\n",
    "    'level':'Intern',\n",
    "    'tweets':'no',\n",
    "}\n",
    "\n",
    "print(ex3)\n",
    "\n",
    "label = classify(tree,ex3)\n",
    "\n",
    "print ('Label:',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: Unexpected/missing data\n",
    "- level: Senior\n",
    "- tweets: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'Senior', 'tweets': 'no'}\n",
      "Label: False\n"
     ]
    }
   ],
   "source": [
    "ex4 = {\n",
    "    'level':'Senior',\n",
    "    'tweets':'no',\n",
    "}\n",
    "\n",
    "print(ex4)\n",
    "\n",
    "label = classify(tree,ex4)\n",
    "\n",
    "print ('Label:',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since our goal was mainly to demonstrate how to build a tree, we built the tree using the entire data set. As always, if we were really trying to create a good model for something, we would have (collected more data and) split the data into train/validation/test subsets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
